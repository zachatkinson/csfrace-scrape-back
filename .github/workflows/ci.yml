name: Progressive CI/CD Pipeline

on:
  push:
    branches: [master, develop]
    paths-ignore:
      - '**.md'
      - 'docs/**'
      - 'LICENSE'
      - '.gitignore'
      - '.pre-commit-config.yaml'
      - 'README.rst'
  pull_request:
    branches: [master]
    paths-ignore:
      - '**.md'
      - 'docs/**' 
      - 'LICENSE'
      - '.gitignore'
      - '.pre-commit-config.yaml'
      - 'README.rst'
  schedule:
    # Run weekly security scans on Sundays at 6 AM UTC
    - cron: '0 6 * * 0'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.13"
  UV_VERSION: "0.8.14"

jobs:
  # Progressive CI: Detect what changed to run only relevant tests
  detect-changes:
    name: Detect Code Changes
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      # Core components
      core-changed: ${{ steps.changes.outputs.core }}
      utils-changed: ${{ steps.changes.outputs.utils }}
      api-changed: ${{ steps.changes.outputs.api }}
      processors-changed: ${{ steps.changes.outputs.processors }}
      rendering-changed: ${{ steps.changes.outputs.rendering }}
      caching-changed: ${{ steps.changes.outputs.caching }}
      database-changed: ${{ steps.changes.outputs.database }}
      batch-changed: ${{ steps.changes.outputs.batch }}
      monitoring-changed: ${{ steps.changes.outputs.monitoring }}
      
      # Test types
      unit-tests-changed: ${{ steps.changes.outputs.unit-tests }}
      integration-tests-changed: ${{ steps.changes.outputs.integration-tests }}
      performance-tests-changed: ${{ steps.changes.outputs.performance-tests }}
      
      # Infrastructure
      docker-changed: ${{ steps.changes.outputs.docker }}
      dependencies-changed: ${{ steps.changes.outputs.dependencies }}
      config-changed: ${{ steps.changes.outputs.config }}
      
      # Overall change detection  
      any-code-changed: ${{ steps.changes.outputs.any-code }}
      force-full-ci: ${{ steps.force-full.outputs.force }}
      
      # Computed conditions for better maintainability
      should-run-unit-tests: ${{ steps.compute-conditions.outputs.unit-tests }}
      should-run-integration-tests: ${{ steps.compute-conditions.outputs.integration-tests }}
      should-run-cross-platform: ${{ steps.compute-conditions.outputs.cross-platform }}
      should-run-performance: ${{ steps.compute-conditions.outputs.performance }}
      should-run-docker: ${{ steps.compute-conditions.outputs.docker }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: false

      - name: Detect changed files
        uses: dorny/paths-filter@v3
        id: changes
        with:
          # Performance optimization: specify base for better git operations
          base: ${{ github.event.repository.default_branch }}
          # Enable detailed file listing for debugging (shell format is most useful)
          list-files: shell  
          # Optimize git fetch depth for better performance
          initial-fetch-depth: 100
          filters: |
            # Core application components (quoted for safety per best practices)
            core:
              - "src/core/**"
              - "src/__init__.py"
              - "src/main.py"
            
            utils:
              - "src/utils/**"
            
            api:
              - "src/api/**"
              - "src/server/**"
            
            processors:
              - "src/processors/**"
            
            rendering:
              - "src/rendering/**"
            
            caching:
              - "src/caching/**"
            
            database:
              - "src/database/**"
              - "alembic/**"
              - "migrations/**"
            
            batch:
              - "src/batch/**"
            
            monitoring:
              - "src/monitoring/**"
            
            # Test files (quoted for safety per best practices)
            unit-tests:
              - "tests/unit/**"
              - "tests/test_*.py"
              - "tests/**/test_*.py"
              - "!tests/integration/**"
              - "!tests/performance/**"
              - "!tests/e2e/**"
            
            integration-tests:
              - "tests/integration/**"
              - "tests/database/**"
            
            performance-tests:
              - "tests/performance/**"
            
            # Infrastructure (quoted for safety per best practices)
            docker:
              - "Dockerfile"
              - "docker-compose*.yml"
              - ".dockerignore"
            
            dependencies:
              - "pyproject.toml"
              - "uv.lock"
              - "requirements/**"
            
            config:
              - ".github/workflows/**"
              - "pyproject.toml"
              - ".pre-commit-config.yaml"
            
            # Catch-all for any code changes (quoted for safety per best practices)
            any-code:
              - "src/**"
              - "tests/**"
              - "pyproject.toml"
              - "uv.lock"
              - "Dockerfile"

      - name: Check for force full CI conditions
        id: force-full
        run: |
          # Force full CI for certain conditions
          force_full_ci=false
          
          # Always run full CI on master branch pushes
          if [[ "${{ github.event_name }}" == "push" && "${{ github.ref }}" == "refs/heads/master" ]]; then
            force_full_ci=true
            echo "::notice title=Progressive CI::Full CI forced - master branch push"
          fi
          
          # Always run full CI on scheduled runs
          if [[ "${{ github.event_name }}" == "schedule" ]]; then
            force_full_ci=true
            echo "::notice title=Progressive CI::Full CI forced - scheduled run"
          fi
          
          # Force full CI if dependencies changed
          if [[ "${{ steps.changes.outputs.dependencies }}" == "true" ]]; then
            force_full_ci=true
            echo "::notice title=Progressive CI::Full CI forced - dependencies changed"
          fi
          
          # Force full CI if config changed
          if [[ "${{ steps.changes.outputs.config }}" == "true" ]]; then
            force_full_ci=true  
            echo "::notice title=Progressive CI::Full CI forced - CI config changed"
          fi
          
          # Check commit message for force flags (using GitHub Actions contains function)
          if ${{ contains(github.event.head_commit.message, '[force ci]') }}; then
            force_full_ci=true
            echo "::notice title=Progressive CI::Full CI forced - [force ci] in commit message"
          fi
          
          echo "force=${force_full_ci}" >> $GITHUB_OUTPUT
          
          # Log what changed for debugging
          echo "::notice title=Progressive CI Changes::Core: ${{ steps.changes.outputs.core }}, Utils: ${{ steps.changes.outputs.utils }}, API: ${{ steps.changes.outputs.api }}, Processors: ${{ steps.changes.outputs.processors }}"

      - name: Compute test execution conditions
        id: compute-conditions
        run: |
          # Using official GitHub Actions best practice: compute complex conditions in a dedicated step
          
          force_full="${{ steps.force-full.outputs.force }}"
          
          # Unit tests: run if unit tests changed, core components changed, or forced
          if [[ "$force_full" == "true" || \
                "${{ steps.changes.outputs.unit-tests }}" == "true" || \
                "${{ steps.changes.outputs.core }}" == "true" || \
                "${{ steps.changes.outputs.utils }}" == "true" || \
                "${{ steps.changes.outputs.processors }}" == "true" ]]; then
            echo "unit-tests=true" >> $GITHUB_OUTPUT
          else
            echo "unit-tests=false" >> $GITHUB_OUTPUT
          fi
          
          # Integration tests: run if integration tests changed or forced
          if [[ "$force_full" == "true" || \
                "${{ steps.changes.outputs.integration-tests }}" == "true" ]]; then
            echo "integration-tests=true" >> $GITHUB_OUTPUT
          else
            echo "integration-tests=false" >> $GITHUB_OUTPUT
          fi
          
          # Cross-platform tests: run if core functionality changed or forced
          if [[ "$force_full" == "true" || \
                "${{ steps.changes.outputs.core }}" == "true" || \
                "${{ steps.changes.outputs.utils }}" == "true" ]]; then
            echo "cross-platform=true" >> $GITHUB_OUTPUT
          else
            echo "cross-platform=false" >> $GITHUB_OUTPUT
          fi
          
          # Performance tests: run if performance code changed, core changed, or forced
          if [[ "$force_full" == "true" || \
                "${{ steps.changes.outputs.performance-tests }}" == "true" || \
                "${{ steps.changes.outputs.core }}" == "true" ]]; then
            echo "performance=true" >> $GITHUB_OUTPUT
          else
            echo "performance=false" >> $GITHUB_OUTPUT
          fi
          
          # Docker scan: run if Docker files changed, dependencies changed, or forced
          if [[ "$force_full" == "true" || \
                "${{ steps.changes.outputs.docker }}" == "true" || \
                "${{ steps.changes.outputs.dependencies }}" == "true" || \
                "${{ github.event_name }}" == "schedule" || \
                "${{ contains(github.event.head_commit.message, '[docker]') }}" == "true" ]]; then
            echo "docker=true" >> $GITHUB_OUTPUT
          else
            echo "docker=false" >> $GITHUB_OUTPUT
          fi
          
          echo "::notice title=Computed Conditions::Unit Tests: $(echo $([[ "$force_full" == "true" || "${{ steps.changes.outputs.unit-tests }}" == "true" || "${{ steps.changes.outputs.core }}" == "true" || "${{ steps.changes.outputs.utils }}" == "true" || "${{ steps.changes.outputs.processors }}" == "true" ]] && echo true || echo false))"
  # Code quality and security checks (foundational job)
  quality:
    name: Code Quality & Security
    runs-on: ubuntu-latest
    needs: detect-changes
    timeout-minutes: 15
    permissions:
      contents: read
      security-events: write  # Required for SARIF upload
      actions: read
    
    # Skip quality checks only if no code changed and not forced
    if: ${{ needs.detect-changes.outputs.any-code-changed == 'true' || needs.detect-changes.outputs.force-full-ci == 'true' }}
    
    outputs:
      quality-passed: ${{ steps.quality-check.outputs.passed }}
      skipped: ${{ steps.quality-check.outputs.skipped || 'false' }}
    
    steps:
      - name: Record job start time
        id: start-time  
        run: echo "start_time=$(date +%s)" >> $GITHUB_OUTPUT
        shell: bash

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: false  # Security best practice

      - name: Install UV (manages Python version automatically)
        uses: astral-sh/setup-uv@v6
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-suffix: quality-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}

      - name: Install dependencies with UV
        run: uv sync --frozen --no-editable

      - name: Run Ruff (auto-fix)
        run: uv run ruff check src/ tests/ --fix
        continue-on-error: true

      - name: Run Ruff (format)
        run: uv run ruff format src/ tests/

      - name: Commit auto-fixes (if any)
        if: github.event_name == 'push'
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          if git diff --exit-code; then
            echo "No auto-fixes applied"
          else
            echo "Auto-fixes applied, committing changes"
            git add -A
            git commit -m "style: apply ruff auto-fixes

            ðŸ¤– Applied by CI auto-fix workflow
            
            Co-Authored-By: GitHub Action <action@github.com>"
            git push
          fi
        continue-on-error: true

      - name: Run Ruff (final linting check)
        run: uv run ruff check src/ tests/ --output-format=github

      - name: Run MyPy (type checking)
        run: uv run mypy src/ --show-error-codes

      - name: Run Bandit (security scan)
        run: uv run bandit -r src/ -f sarif -o bandit-report.sarif
        continue-on-error: true

      - name: Upload Bandit SARIF results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: bandit-report.sarif
          category: bandit

      - name: Run Safety (dependency security)
        run: |
          # Safety 3.x requires authentication, fall back to pip-audit if unavailable
          if [[ -n "$SAFETY_API_KEY" ]]; then
            uv run safety scan --output json --save-json safety-report.json || 
            echo '{"vulnerabilities": [], "note": "Safety scan failed - using pip-audit instead"}' > safety-report.json
          else
            echo '{"vulnerabilities": [], "note": "Safety 3.x requires API key - using pip-audit for vulnerability scanning"}' > safety-report.json
            echo "::notice title=Safety Scanner::Safety 3.x requires authentication - using pip-audit for vulnerability scanning"
          fi
        continue-on-error: true

      - name: Run pip-audit (package vulnerabilities) 
        run: uv run pip-audit --format=json --output=pip-audit-report.json

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            safety-report.json
            pip-audit-report.json
          retention-days: 30

      - name: Calculate job duration and set outputs
        id: quality-check
        run: |
          end_time=$(date +%s)
          duration=$((end_time - ${{ steps.start-time.outputs.start_time }}))
          echo "passed=true" >> $GITHUB_OUTPUT
          echo "duration=${duration}" >> $GITHUB_OUTPUT
          echo "::notice title=Quality Job Duration::Quality checks completed in ${duration} seconds"
        shell: bash

  # Unit Tests Matrix (optimized platform strategy)
  # Ubuntu: Full test suite with coverage (primary validation) + PostgreSQL service
  # Windows/macOS: Platform-specific smoke tests (compatibility validation)
  unit-tests-linux:
    name: Unit Tests - Ubuntu (Shard ${{ matrix.shard }})
    runs-on: ubuntu-latest
    needs: [detect-changes, quality]
    timeout-minutes: 25
    
    # Use computed condition for cleaner logic (GitHub Actions best practice)
    if: ${{ always() && (needs.quality.result == 'success' || needs.quality.result == 'skipped') && needs.detect-changes.outputs.should-run-unit-tests == 'true' }}
    permissions:
      contents: read
      checks: write
    
    # PostgreSQL service container for database unit tests
    services:
      postgres:
        # Using official postgres image - minimal configuration per GitHub Actions docs
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres  
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    strategy:
      fail-fast: false
      matrix:
        # Enable 4-way test sharding following PostgreSQL best practices
        # PostgreSQL official docs recommend max 20 parallel test scripts (40 processes)
        # GitHub Actions runners have 2-4 cores, so 4 shards aligns with both constraints
        shard: [1, 2, 3, 4]

    steps:
      - name: Record job start time
        id: start-time
        run: echo "start_time=$(date +%s)" >> $GITHUB_OUTPUT
        shell: bash

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Install UV (manages Python version automatically)
        uses: astral-sh/setup-uv@v6
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-suffix: unit-linux-shard-${{ matrix.shard }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}

      - name: Cache test durations for optimal shard balancing
        uses: actions/cache@v4
        with:
          path: .pytest_cache/durations.json
          key: pytest-durations-${{ runner.os }}-${{ hashFiles('tests/**/*.py') }}
          restore-keys: |
            pytest-durations-${{ runner.os }}-
      
      - name: Create pytest cache directory
        run: |
          mkdir -p .pytest_cache
          # Create empty durations file to prevent cache warning
          touch .pytest_cache/durations.json

      - name: Install dependencies with UV
        run: |
          uv sync --frozen --no-editable
          # Install pytest-split for optimal matrix-based test sharding
          uv add pytest-split --group=test

      - name: Install Playwright browsers (Shard 4 only - dedicated rendering shard)
        if: matrix.shard == 4
        run: |
          # Shard 4 is the dedicated rendering shard for all Playwright tests
          # Following official Playwright recommendations - no caching
          # Caching adds complexity without meaningful benefit per official docs
          # Fresh install ensures version compatibility
          uv run playwright install --with-deps chromium
        
      - name: Create shard-specific database
        run: |
          # Create isolated database for this shard to eliminate data bleeding
          PGPASSWORD=postgres psql -h localhost -U postgres -d postgres -c "CREATE DATABASE test_db_shard_${{ matrix.shard }};" || true
          echo "Created database test_db_shard_${{ matrix.shard }}"
        
      - name: Run comprehensive unit tests with coverage (Ubuntu with PostgreSQL)
        env:
          # PostgreSQL connection settings for database unit tests
          # Each shard uses its own database to eliminate data bleeding
          TEST_DATABASE_URL: postgresql+psycopg://postgres:postgres@localhost:5432/test_db_shard_${{ matrix.shard }}
          DATABASE_URL: postgresql+psycopg://postgres:postgres@localhost:5432/test_db_shard_${{ matrix.shard }}
          # Override default credentials to match service container
          DATABASE_HOST: localhost
          DATABASE_PORT: 5432
          DATABASE_NAME: test_db_shard_${{ matrix.shard }}
          DATABASE_USER: postgres
          DATABASE_PASSWORD: postgres
        run: |
          # Run tests using pytest-split for optimal matrix-based sharding (official best practice)
          # pytest-split distributes tests evenly across shards with duration-based balancing
          # Each shard runs its subset with internal parallelization via pytest-xdist
          
          # Shard-specific test execution for dedicated Playwright optimization
          if [ "${{ matrix.shard }}" = "4" ]; then
            echo "::notice title=Shard ${{ matrix.shard }}::Dedicated rendering shard - includes Playwright tests"
            # Shard 4: Dedicated rendering shard (includes tests/rendering/)
            uv run pytest tests/ \
              --ignore=tests/integration/ \
              --ignore=tests/performance/ \
              --ignore=tests/e2e/ \
              --ignore=tests/api/ \
              -m "not integration" \
              --splits=4 \
              --group=${{ matrix.shard }} \
              -n auto \
              --dist=worksteal \
              --durations=10 \
              --cov=src \
              --cov-branch \
            --cov-report=xml:coverage-shard-${{ matrix.shard }}.xml \
            --cov-report=term-missing \
            --junit-xml=junit-unit-${{ matrix.python-version }}-shard-${{ matrix.shard }}.xml \
            -v \
              --tb=short \
              --maxfail=3
          else
            echo "::notice title=Shard ${{ matrix.shard }}::Standard unit tests shard - excludes rendering tests for speed"
            # Shards 1-3: Exclude rendering tests (no Playwright needed)
            uv run pytest tests/ \
              --ignore=tests/integration/ \
              --ignore=tests/performance/ \
              --ignore=tests/e2e/ \
              --ignore=tests/api/ \
              --ignore=tests/rendering/ \
              -m "not integration" \
              --splits=4 \
              --group=${{ matrix.shard }} \
              -n auto \
              --dist=worksteal \
              --durations=10 \
              --cov=src \
              --cov-branch \
              --cov-report=xml:coverage-shard-${{ matrix.shard }}.xml \
              --cov-report=term-missing \
              --junit-xml=junit-unit-${{ matrix.python-version }}-shard-${{ matrix.shard }}.xml \
              -v \
              --tb=short \
              --maxfail=3
          fi
          
          # Verify minimum coverage threshold
          uv run coverage report --fail-under=28

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-unit-ubuntu-py${{ matrix.python-version }}-shard-${{ matrix.shard }}
          path: |
            junit-unit-${{ matrix.python-version }}-shard-${{ matrix.shard }}.xml
            coverage-shard-${{ matrix.shard }}.xml
          retention-days: 30

      - name: Upload coverage reports to Codecov (Ubuntu only - primary validation)
        uses: codecov/codecov-action@v5
        if: matrix.python-version == env.PYTHON_VERSION
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./coverage-shard-${{ matrix.shard }}.xml
          flags: unit-tests
          name: unit-tests-coverage
          slug: zachatkinson/csfrace-scrape-back
          fail_ci_if_error: false

      - name: Upload test results to Codecov (Test Analytics)
        uses: codecov/test-results-action@v1
        if: ${{ !cancelled() && matrix.python-version == env.PYTHON_VERSION }}
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./junit-unit-${{ matrix.python-version }}-shard-${{ matrix.shard }}.xml
          flags: unit-tests
          name: unit-tests-results
          fail_ci_if_error: false

      - name: Calculate and report job duration
        if: always()
        run: |
          end_time=$(date +%s)
          duration=$((end_time - ${{ steps.start-time.outputs.start_time }}))
          echo "::notice title=Unit Tests Duration (Ubuntu)::Unit tests completed in ${duration} seconds"
        shell: bash

  # Cross-platform compatibility tests (Windows/macOS)
  unit-tests-cross-platform:
    name: Unit Tests - ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    needs: [detect-changes, quality]
    timeout-minutes: 15
    
    # Use computed condition for cleaner logic (GitHub Actions best practice)
    if: ${{ always() && (needs.quality.result == 'success' || needs.quality.result == 'skipped') && needs.detect-changes.outputs.should-run-cross-platform == 'true' }}
    permissions:
      contents: read
      checks: write
    
    strategy:
      fail-fast: false
      matrix:
        os: [windows-latest, macos-latest]

    steps:
      - name: Record job start time
        id: start-time
        run: echo "start_time=$(date +%s)" >> $GITHUB_OUTPUT
        shell: bash

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Install UV (manages Python version automatically)
        uses: astral-sh/setup-uv@v6
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-suffix: unit-cross-${{ matrix.os }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}

      - name: Install dependencies with UV
        run: uv sync --frozen --no-editable
        
      - name: Run platform-specific smoke tests (Windows/macOS - compatibility validation)
        run: |
          # SMOKE TESTS ONLY - comprehensive testing handled by Ubuntu shards
          # Focus on platform-specific compatibility and core functionality validation
          # Exclude tests/unit/ entirely (covered by Ubuntu shards) to avoid duplication
          uv run pytest tests/test_main.py tests/utils/test_http.py tests/config/ tests/caching/test_file_cache.py -k "not integration and not performance and not e2e and not database and not slow" -x --tb=short --maxfail=3 --junit-xml=junit-smoke-${{ matrix.os }}.xml -v
          
          # Verify core imports work on platform
          uv run python -c "import src.main; import src.core.converter; print('Core imports successful')"

      - name: Upload smoke test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: smoke-test-results-${{ matrix.os }}
          path: junit-smoke-${{ matrix.os }}.xml
          retention-days: 30

      - name: Calculate and report job duration
        if: always()
        run: |
          end_time=$(date +%s)
          duration=$((end_time - ${{ steps.start-time.outputs.start_time }}))
          echo "::notice title=Unit Tests Duration (${{ matrix.os }})::Unit tests completed in ${duration} seconds"
        shell: bash

  # Integration Tests (Redis, Database, Converter) - Run in parallel with unit tests
  # Database isolation with SQLAlchemy 2.0 SAVEPOINT patterns enables safe parallel execution
  integration-tests:
    name: Integration Tests - ${{ matrix.test-type }}
    runs-on: ubuntu-latest
    needs: [detect-changes, quality]  # Run IN PARALLEL with unit tests - database isolation is now perfect
    timeout-minutes: 15
    
    # Use computed condition for cleaner logic (GitHub Actions best practice)
    if: ${{ always() && (needs.quality.result == 'success' || needs.quality.result == 'skipped') && needs.detect-changes.outputs.should-run-integration-tests == 'true' }}
    permissions:
      contents: read
      checks: write
      security-events: write
    
    strategy:
      fail-fast: false
      matrix:
        test-type: ["redis", "database", "converter"]
        include:
          - test-type: "redis"
            services: '{"redis": {"image": "redis:7-alpine", "ports": ["6379:6379"], "options": "--health-cmd \"redis-cli ping\" --health-interval 10s --health-timeout 5s --health-retries 5"}}'
            env-vars: "REDIS_URL=redis://localhost:6379"
            test-path: "tests/integration/test_redis_cache.py"
          - test-type: "database" 
            services: '{"postgres": {"image": "postgres:17.6", "env": {"POSTGRES_DB": "test_db", "POSTGRES_USER": "test_user", "POSTGRES_PASSWORD": "test_password"}, "ports": ["5432:5432"], "options": "--health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5"}}'
            env-vars: "DATABASE_HOST=localhost DATABASE_PORT=5432 DATABASE_NAME=test_db DATABASE_USER=test_user DATABASE_PASSWORD=test_password"
            test-path: "tests/database/"
          - test-type: "converter"
            services: "{}"
            env-vars: ""
            test-path: "tests/integration/test_converter_integration.py"

    services: ${{ fromJSON(matrix.services) }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Install UV (manages Python version automatically)
        uses: astral-sh/setup-uv@v6
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-suffix: integration-${{ matrix.test-type }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}

      - name: Install system dependencies
        run: |
          sudo apt-get update
          if [ "${{ matrix.test-type }}" = "redis" ]; then
            sudo apt-get install -y redis-tools
          elif [ "${{ matrix.test-type }}" = "database" ]; then
            sudo apt-get install -y postgresql-client
          fi
          
      - name: Install dependencies with UV
        run: uv sync --frozen --no-editable

      - name: Verify service connections
        run: |
          if [ "${{ matrix.test-type }}" = "redis" ]; then
            redis-cli -h localhost -p 6379 ping
          elif [ "${{ matrix.test-type }}" = "database" ]; then
            pg_isready -h localhost -p 5432 -U test_user -d test_db
            psql -h localhost -p 5432 -U test_user -d test_db -c "SELECT version();"
          fi
        env:
          PGPASSWORD: test_password

      - name: Run integration tests
        run: |
          if [ "${{ matrix.test-type }}" = "database" ]; then
            uv run pytest ${{ matrix.test-path }} -m "integration" --junit-xml=junit-${{ matrix.test-type }}-integration.xml -v --tb=short --maxfail=5
          else
            uv run pytest ${{ matrix.test-path }} --junit-xml=junit-${{ matrix.test-type }}-integration.xml -v --tb=short --maxfail=5
          fi
        env:
          REDIS_URL: ${{ matrix.test-type == 'redis' && 'redis://localhost:6379' || '' }}
          DATABASE_HOST: ${{ matrix.test-type == 'database' && 'localhost' || '' }}
          DATABASE_PORT: ${{ matrix.test-type == 'database' && '5432' || '' }}
          DATABASE_NAME: ${{ matrix.test-type == 'database' && 'test_db' || '' }}
          DATABASE_USER: ${{ matrix.test-type == 'database' && 'test_user' || '' }}
          DATABASE_PASSWORD: ${{ matrix.test-type == 'database' && 'test_password' || '' }}

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.test-type }}-integration
          path: junit-${{ matrix.test-type }}-integration.xml
          retention-days: 30

      - name: Upload integration test results to Codecov (Test Analytics)
        uses: codecov/test-results-action@v1
        if: ${{ !cancelled() }}
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./junit-${{ matrix.test-type }}-integration.xml
          flags: integration-tests
          name: integration-tests-${{ matrix.test-type }}
          fail_ci_if_error: false

      - name: Calculate and report job duration
        if: always()
        run: |
          if [ -n "${{ github.run_started_at }}" ]; then
            # Calculate duration from workflow start for parallel jobs
            start_epoch=$(date -d "${{ github.event.head_commit.timestamp }}" +%s 2>/dev/null || echo "$(date +%s)")
            end_time=$(date +%s)
            duration=$((end_time - start_epoch))
            echo "::notice title=Integration Tests Duration (${{ matrix.test-type }})::${{ matrix.test-type }} integration tests completed in ${duration} seconds"
          fi
        shell: bash

  # Removed dependency compatibility jobs - using uv.lock for reproducible builds
  # Trust modern dependency management instead of redundant version testing

  # Performance Benchmarks
  performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [detect-changes, quality, unit-tests-linux, unit-tests-cross-platform, integration-tests]
    timeout-minutes: 60
    
    # Concurrency control for performance jobs (GitHub Actions best practice)
    concurrency:
      group: performance-${{ github.ref }}
      cancel-in-progress: false  # Don't cancel performance tests as they take time to run
    
    # Optimized permissions for benchmark uploads (GitHub Actions best practice)
    permissions:
      contents: write  # Required to create gh-pages branch
      actions: write   # For benchmark data uploads
      pages: write     # For GitHub Pages deployment
    
    # Use computed condition for cleaner logic (GitHub Actions best practice)
    if: ${{ !contains(github.event.head_commit.message, '[skip performance]') && needs.detect-changes.outputs.should-run-performance == 'true' && (github.event_name == 'push' || github.event.pull_request.head.repo.full_name == github.repository) }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          # GitHub Actions best practice: allow token for gh-pages operations
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0  # Full history needed for benchmark comparisons

      - name: Install UV (manages Python version automatically)
        uses: astral-sh/setup-uv@v6
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-suffix: performance-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}

      - name: Install dependencies with UV
        run: uv sync --frozen --no-editable

      - name: Install Playwright browsers
        run: |
          # Following official Playwright recommendations - no caching
          # Caching adds complexity without meaningful benefit per official docs
          # Fresh install ensures version compatibility
          uv run playwright install --with-deps chromium


      - name: Run performance tests
        run: |
          # Run all performance tests including memory profiling and benchmarks
          # Don't use --benchmark-only as it skips non-benchmark tests (memory profiler, cache tests)
          # Remove --benchmark-compare-fail for initial runs (requires baseline comparison)
          uv run pytest tests/performance/ --benchmark-json=benchmark.json --benchmark-sort=mean --junit-xml=junit-performance.xml -v
          
          # Generate human-readable benchmark report
          echo "## ðŸš€ Performance Benchmark Results" > benchmark-report.md
          echo "" >> benchmark-report.md
          
          # Extract key metrics from benchmark.json for reporting
          uv run python -c "
          import json
          
          with open('benchmark.json', 'r') as f:
              data = json.load(f)
          
          print('| Test | Mean Time | Min Time | Max Time |')
          print('|------|-----------|----------|----------|')
          
          for benchmark in data.get('benchmarks', []):
              name = benchmark['name']
              stats = benchmark['stats']
              mean = f\"{stats['mean']:.4f}s\"
              min_time = f\"{stats['min']:.4f}s\"
              max_time = f\"{stats['max']:.4f}s\"
              print(f'| {name} | {mean} | {min_time} | {max_time} |')
          " >> benchmark-report.md

      - name: Upload performance test results to Codecov (Test Analytics)
        uses: codecov/test-results-action@v1
        if: ${{ !cancelled() }}
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./junit-performance.xml
          flags: performance-tests

      # Store benchmark results as artifacts for historical comparison
      - name: Store benchmark baseline (master branch)
        if: github.event_name == 'push' && github.ref == 'refs/heads/master'
        run: |
          # Create timestamped benchmark file for historical tracking
          timestamp=$(date +%Y%m%d_%H%M%S)
          commit_sha=$(git rev-parse --short HEAD)
          cp benchmark.json "benchmark_${timestamp}_${commit_sha}.json"
          
          echo "## ðŸ“ˆ Performance Benchmark Results" >> benchmark_summary.md
          echo "**Commit:** ${commit_sha}" >> benchmark_summary.md
          echo "**Timestamp:** $(date)" >> benchmark_summary.md
          echo "" >> benchmark_summary.md
          echo "### Benchmark Data" >> benchmark_summary.md
          echo '```json' >> benchmark_summary.md
          cat benchmark.json >> benchmark_summary.md
          echo '```' >> benchmark_summary.md

      - name: Compare performance (PRs)
        if: github.event_name == 'pull_request'
        run: |
          # For PRs, we would download latest master benchmark from artifacts
          # and compare - this requires more setup but provides real regression detection
          echo "PR benchmark comparison would go here"
          echo "Current benchmark:" 
          cat benchmark.json

      - name: Upload benchmark results and report
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmarks-${{ github.run_number }}
          path: |
            benchmark.json
            benchmark-report.md
            junit-performance.xml
            benchmark_*.json
            benchmark_summary.md
          retention-days: 90  # Maximum allowed by repository policy

      - name: Calculate and report job duration
        if: always()
        run: |
          if [ -n "${{ github.run_started_at }}" ]; then
            start_epoch=$(date -d "${{ github.event.head_commit.timestamp }}" +%s 2>/dev/null || echo "$(date +%s)")
            end_time=$(date +%s)
            duration=$((end_time - start_epoch))
            echo "::notice title=Performance Tests Duration::Performance benchmarks completed in ${duration} seconds"
          fi
        shell: bash

  # Docker Security Scan (pinned versions, proper error handling)
  docker-security:
    name: Docker Build & Security Scan
    runs-on: ubuntu-latest
    needs: [detect-changes, quality]
    timeout-minutes: 15
    # Use computed condition for cleaner logic (GitHub Actions best practice)  
    if: ${{ !contains(github.event.head_commit.message, '[skip docker]') && needs.detect-changes.outputs.should-run-docker == 'true' }}
    permissions:
      contents: read
      security-events: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: false
          load: true
          tags: csfrace-scraper:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Run Trivy vulnerability scanner (fail on HIGH/CRITICAL)
        uses: aquasecurity/trivy-action@0.16.1  # Pinned version for security
        with:
          image-ref: 'csfrace-scraper:${{ github.sha }}'
          format: 'table'
          severity: 'HIGH,CRITICAL'
          exit-code: 1
          
      - name: Run Trivy vulnerability scanner (SARIF output)
        uses: aquasecurity/trivy-action@0.16.1
        if: always()
        with:
          image-ref: 'csfrace-scraper:${{ github.sha }}'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'HIGH,CRITICAL'
        continue-on-error: true

      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'
          category: trivy

      - name: Run Hadolint (Dockerfile linting)
        uses: hadolint/hadolint-action@v3.1.0
        with:
          dockerfile: Dockerfile
          format: sarif
          output-file: hadolint-results.sarif
        continue-on-error: true

      - name: Upload Hadolint results
        uses: github/codeql-action/upload-sarif@v3
        if: always() && hashFiles('hadolint-results.sarif') != ''
        with:
          sarif_file: hadolint-results.sarif
          category: hadolint

  # Dependency Review (PR only)
  dependency-review:
    name: Dependency Security Review
    runs-on: ubuntu-latest
    timeout-minutes: 5
    permissions:
      contents: read
      pull-requests: read
    
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false
          
      - name: Dependency Review
        uses: actions/dependency-review-action@v3
        with:
          fail-on-severity: moderate
          allow-licenses: Apache-2.0, BSD-2-Clause, BSD-3-Clause, ISC, MIT