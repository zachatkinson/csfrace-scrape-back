name: CI/CD Pipeline

on:
  push:
    branches: [master, develop]
    paths-ignore:
      - '**.md'
      - 'docs/**'
      - 'LICENSE'
      - '.gitignore'
      - '.pre-commit-config.yaml'
      - 'README.rst'
  pull_request:
    branches: [master]
    paths-ignore:
      - '**.md'
      - 'docs/**' 
      - 'LICENSE'
      - '.gitignore'
      - '.pre-commit-config.yaml'
      - 'README.rst'
  schedule:
    # Run weekly security scans on Sundays at 6 AM UTC
    - cron: '0 6 * * 0'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.13"
  UV_VERSION: "0.8.14"

jobs:
  # Code quality and security checks (foundational job)
  quality:
    name: Code Quality & Security
    runs-on: ubuntu-latest
    timeout-minutes: 15
    permissions:
      contents: read
      security-events: write  # Required for SARIF upload
      actions: read
    
    outputs:
      quality-passed: ${{ steps.quality-check.outputs.passed }}
    
    steps:
      - name: Record job start time
        id: start-time  
        run: echo "start_time=$(date +%s)" >> $GITHUB_OUTPUT
        shell: bash

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: false  # Security best practice

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install UV
        uses: astral-sh/setup-uv@v6
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-suffix: quality-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}

      - name: Install dependencies with UV
        run: uv sync --frozen --no-editable

      - name: Run Ruff (linting)
        run: uv run ruff check src/ tests/ --output-format=github

      - name: Run Ruff (formatting check)
        run: uv run ruff format --check src/ tests/

      - name: Run MyPy (type checking)
        run: uv run mypy src/ --show-error-codes

      - name: Run Bandit (security scan)
        run: uv run bandit -r src/ -f sarif -o bandit-report.sarif
        continue-on-error: true

      - name: Upload Bandit SARIF results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: bandit-report.sarif
          category: bandit

      - name: Run Safety (dependency security)
        run: |
          # Safety 3.x requires authentication, fall back to pip-audit if unavailable
          if [[ -n "$SAFETY_API_KEY" ]]; then
            uv run safety scan --output json --save-json safety-report.json || 
            echo '{"vulnerabilities": [], "note": "Safety scan failed - using pip-audit instead"}' > safety-report.json
          else
            echo '{"vulnerabilities": [], "note": "Safety 3.x requires API key - using pip-audit for vulnerability scanning"}' > safety-report.json
            echo "::notice title=Safety Scanner::Safety 3.x requires authentication - using pip-audit for vulnerability scanning"
          fi
        continue-on-error: true

      - name: Run pip-audit (package vulnerabilities) 
        run: uv run pip-audit --format=json --output=pip-audit-report.json

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            safety-report.json
            pip-audit-report.json
          retention-days: 30

      - name: Calculate job duration and set outputs
        id: quality-check
        run: |
          end_time=$(date +%s)
          duration=$((end_time - ${{ steps.start-time.outputs.start_time }}))
          echo "passed=true" >> $GITHUB_OUTPUT
          echo "duration=${duration}" >> $GITHUB_OUTPUT
          echo "::notice title=Quality Job Duration::Quality checks completed in ${duration} seconds"
        shell: bash

  # Unit Tests Matrix (optimized platform strategy)
  # Ubuntu: Full test suite with coverage (primary validation) + PostgreSQL service
  # Windows/macOS: Platform-specific smoke tests (compatibility validation)
  unit-tests-linux:
    name: Unit Tests - Ubuntu Python ${{ matrix.python-version }} (Shard ${{ matrix.shard }})
    runs-on: ubuntu-latest
    needs: quality
    timeout-minutes: 25
    permissions:
      contents: read
      checks: write
    
    # PostgreSQL service container for database unit tests
    services:
      postgres:
        # Using official Docker Hub postgres image (GitHub Actions best practice)
        image: postgres:13
        env:
          # Required environment variables per Docker official docs
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: test_db
          # PostgreSQL initialization arguments for CI optimization
          POSTGRES_INITDB_ARGS: >-
            --auth-host=md5
            --shared_preload_libraries=''
        # PostgreSQL configuration per Docker official image docs
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          -e POSTGRES_PASSWORD=postgres
          -e POSTGRES_USER=postgres
          -e POSTGRES_DB=test_db
        ports:
          # Explicit port mapping for runner-based jobs (GitHub Actions best practice)
          - 5432:5432
    
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.13"]
        # Enable 4-way test sharding following PostgreSQL best practices
        # PostgreSQL official docs recommend max 20 parallel test scripts (40 processes)
        # GitHub Actions runners have 2-4 cores, so 4 shards aligns with both constraints
        shard: [1, 2, 3, 4]

    steps:
      - name: Record job start time
        id: start-time
        run: echo "start_time=$(date +%s)" >> $GITHUB_OUTPUT
        shell: bash

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v6
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-suffix: unit-linux-${{ matrix.python-version }}-shard-${{ matrix.shard }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}

      - name: Install dependencies with UV
        run: |
          uv sync --frozen --no-editable
          # Install pytest-split for optimal matrix-based test sharding
          uv add pytest-split --group=test

      - name: Install Playwright browsers
        run: |
          # Following official Playwright recommendations - no caching
          # Caching adds complexity without meaningful benefit per official docs
          # Fresh install ensures version compatibility
          uv run playwright install --with-deps chromium
        
      - name: Create shard-specific database
        run: |
          # Create isolated database for this shard to eliminate data bleeding
          PGPASSWORD=postgres psql -h localhost -U postgres -d postgres -c "CREATE DATABASE test_db_shard_${{ matrix.shard }};" || true
          echo "Created database test_db_shard_${{ matrix.shard }}"
        
      - name: Run comprehensive unit tests with coverage (Ubuntu with PostgreSQL)
        env:
          # PostgreSQL connection settings for database unit tests
          # Each shard uses its own database to eliminate data bleeding
          TEST_DATABASE_URL: postgresql+psycopg://postgres:postgres@localhost:5432/test_db_shard_${{ matrix.shard }}
          DATABASE_URL: postgresql+psycopg://postgres:postgres@localhost:5432/test_db_shard_${{ matrix.shard }}
          # Override default credentials to match service container
          DATABASE_HOST: localhost
          DATABASE_PORT: 5432
          DATABASE_NAME: test_db_shard_${{ matrix.shard }}
          DATABASE_USER: postgres
          DATABASE_PASSWORD: postgres
        run: |
          # Run tests using pytest-split for optimal matrix-based sharding (official best practice)
          # pytest-split distributes tests evenly across shards with duration-based balancing
          # Each shard runs its subset with internal parallelization via pytest-xdist
          uv run pytest tests/ \
            --ignore=tests/integration/ \
            --ignore=tests/performance/ \
            --ignore=tests/e2e/ \
            --ignore=tests/api/ \
            --splits=8 \
            --group=${{ matrix.shard }} \
            -n auto \
            --dist=worksteal \
            --durations=10 \
            --cov=src \
            --cov-branch \
            --cov-report=xml:coverage-shard-${{ matrix.shard }}.xml \
            --cov-report=term-missing \
            --junit-xml=junit-unit-${{ matrix.python-version }}-shard-${{ matrix.shard }}.xml \
            -v \
            --tb=short \
            --maxfail=3
          
          # Verify minimum coverage threshold
          uv run coverage report --fail-under=28

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-unit-ubuntu-py${{ matrix.python-version }}-shard-${{ matrix.shard }}
          path: |
            junit-unit-${{ matrix.python-version }}-shard-${{ matrix.shard }}.xml
            coverage-shard-${{ matrix.shard }}.xml
          retention-days: 30

      - name: Upload coverage reports to Codecov (Ubuntu only - primary validation)
        uses: codecov/codecov-action@v3
        if: matrix.python-version == env.PYTHON_VERSION
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          files: ./coverage-shard-${{ matrix.shard }}.xml
          flags: unit-tests
          name: unit-tests-coverage
          fail_ci_if_error: false

      - name: Calculate and report job duration
        if: always()
        run: |
          end_time=$(date +%s)
          duration=$((end_time - ${{ steps.start-time.outputs.start_time }}))
          echo "::notice title=Unit Tests Duration (Ubuntu)::Unit tests completed in ${duration} seconds"
        shell: bash

  # Cross-platform compatibility tests (Windows/macOS)
  unit-tests-cross-platform:
    name: Unit Tests - ${{ matrix.os }} Python ${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}
    needs: quality
    timeout-minutes: 15
    permissions:
      contents: read
      checks: write
    
    strategy:
      fail-fast: false
      matrix:
        os: [windows-latest, macos-latest]
        python-version: ["3.13"]

    steps:
      - name: Record job start time
        id: start-time
        run: echo "start_time=$(date +%s)" >> $GITHUB_OUTPUT
        shell: bash

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v6
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-suffix: unit-cross-${{ matrix.os }}-${{ matrix.python-version }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}

      - name: Install dependencies with UV
        run: uv sync --frozen --no-editable

      - name: Install Playwright browsers
        run: |
          # Following official Playwright recommendations - no caching
          # Caching adds complexity without meaningful benefit per official docs
          # Fresh install ensures version compatibility
          uv run playwright install --with-deps chromium
        
      - name: Run platform-specific smoke tests (Windows/macOS - compatibility validation)
        run: |
          # Platform-specific tests with parallel execution
          # Exclude database tests since PostgreSQL service containers are not available
          # Reduced test scope for faster feedback on platform compatibility
          uv run pytest tests/utils/ tests/config/ tests/caching/test_file_cache.py tests/batch/test_processor.py tests/test_main.py tests/unit/ -k "not integration and not performance and not e2e and not database" -n auto --dist=worksteal --junit-xml=junit-unit-${{ matrix.python-version }}.xml -v --tb=short --maxfail=5

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-unit-${{ matrix.os }}-py${{ matrix.python-version }}
          path: junit-unit*-${{ matrix.python-version }}.xml
          retention-days: 30

      - name: Calculate and report job duration
        if: always()
        run: |
          end_time=$(date +%s)
          duration=$((end_time - ${{ steps.start-time.outputs.start_time }}))
          echo "::notice title=Unit Tests Duration (${{ matrix.os }})::Unit tests completed in ${duration} seconds"
        shell: bash

  # Parallel Integration Tests (Redis, Database, Converter)
  integration-tests:
    name: Integration Tests - ${{ matrix.test-type }}
    runs-on: ubuntu-latest
    needs: quality
    timeout-minutes: 15
    permissions:
      contents: read
      checks: write
      security-events: write
    
    strategy:
      fail-fast: false
      matrix:
        test-type: ["redis", "database", "converter"]
        include:
          - test-type: "redis"
            services: '{"redis": {"image": "redis:7-alpine", "ports": ["6379:6379"], "options": "--health-cmd \"redis-cli ping\" --health-interval 10s --health-timeout 5s --health-retries 5"}}'
            env-vars: "REDIS_URL=redis://localhost:6379"
            test-path: "tests/integration/test_redis_cache.py"
          - test-type: "database" 
            services: '{"postgres": {"image": "postgres:17.6", "env": {"POSTGRES_DB": "test_db", "POSTGRES_USER": "test_user", "POSTGRES_PASSWORD": "test_password"}, "ports": ["5432:5432"], "options": "--health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5"}}'
            env-vars: "DATABASE_HOST=localhost DATABASE_PORT=5432 DATABASE_NAME=test_db DATABASE_USER=test_user DATABASE_PASSWORD=test_password"
            test-path: "tests/database/"
          - test-type: "converter"
            services: "{}"
            env-vars: ""
            test-path: "tests/integration/test_converter_integration.py"

    services: ${{ fromJSON(matrix.services) }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install UV
        uses: astral-sh/setup-uv@v6
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-suffix: integration-${{ matrix.test-type }}-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}

      - name: Install system dependencies
        run: |
          sudo apt-get update
          if [ "${{ matrix.test-type }}" = "redis" ]; then
            sudo apt-get install -y redis-tools
          elif [ "${{ matrix.test-type }}" = "database" ]; then
            sudo apt-get install -y postgresql-client
          fi
          
      - name: Install dependencies with UV
        run: uv sync --frozen --no-editable

      - name: Verify service connections
        run: |
          if [ "${{ matrix.test-type }}" = "redis" ]; then
            redis-cli -h localhost -p 6379 ping
          elif [ "${{ matrix.test-type }}" = "database" ]; then
            pg_isready -h localhost -p 5432 -U test_user -d test_db
            psql -h localhost -p 5432 -U test_user -d test_db -c "SELECT version();"
          fi
        env:
          PGPASSWORD: test_password

      - name: Run integration tests
        run: |
          if [ "${{ matrix.test-type }}" = "database" ]; then
            uv run pytest ${{ matrix.test-path }} -m "database" --junit-xml=junit-${{ matrix.test-type }}-integration.xml -v --tb=short --maxfail=5
          else
            uv run pytest ${{ matrix.test-path }} --junit-xml=junit-${{ matrix.test-type }}-integration.xml -v --tb=short --maxfail=5
          fi
        env:
          REDIS_URL: ${{ matrix.test-type == 'redis' && 'redis://localhost:6379' || '' }}
          DATABASE_HOST: ${{ matrix.test-type == 'database' && 'localhost' || '' }}
          DATABASE_PORT: ${{ matrix.test-type == 'database' && '5432' || '' }}
          DATABASE_NAME: ${{ matrix.test-type == 'database' && 'test_db' || '' }}
          DATABASE_USER: ${{ matrix.test-type == 'database' && 'test_user' || '' }}
          DATABASE_PASSWORD: ${{ matrix.test-type == 'database' && 'test_password' || '' }}

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.test-type }}-integration
          path: junit-${{ matrix.test-type }}-integration.xml
          retention-days: 30

      - name: Calculate and report job duration
        if: always()
        run: |
          if [ -n "${{ github.run_started_at }}" ]; then
            # Calculate duration from workflow start for parallel jobs
            start_epoch=$(date -d "${{ github.event.head_commit.timestamp }}" +%s 2>/dev/null || echo "$(date +%s)")
            end_time=$(date +%s)
            duration=$((end_time - start_epoch))
            echo "::notice title=Integration Tests Duration (${{ matrix.test-type }})::${{ matrix.test-type }} integration tests completed in ${duration} seconds"
          fi
        shell: bash

  # Removed dependency compatibility jobs - using uv.lock for reproducible builds
  # Trust modern dependency management instead of redundant version testing

  # Performance Benchmarks
  performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [quality, unit-tests-linux, unit-tests-cross-platform, integration-tests]
    timeout-minutes: 60
    permissions:
      contents: read
    
    if: ${{ !contains(github.event.head_commit.message, '[skip performance]') && (github.event_name == 'push' || github.event.pull_request.head.repo.full_name == github.repository) }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install UV
        uses: astral-sh/setup-uv@v6
        with:
          version: ${{ env.UV_VERSION }}
          enable-cache: true
          cache-suffix: performance-${{ hashFiles('**/uv.lock', '**/pyproject.toml') }}

      - name: Install dependencies with UV
        run: uv sync --frozen --no-editable

      - name: Install Playwright browsers
        run: |
          # Following official Playwright recommendations - no caching
          # Caching adds complexity without meaningful benefit per official docs
          # Fresh install ensures version compatibility
          uv run playwright install --with-deps chromium

      - name: Run performance tests
        run: |
          # Run all performance tests including memory profiling and benchmarks
          # Don't use --benchmark-only as it skips non-benchmark tests (memory profiler, cache tests)
          uv run pytest tests/performance/ --benchmark-json=benchmark.json --benchmark-sort=mean -v

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark.json
          retention-days: 90

      - name: Calculate and report job duration
        if: always()
        run: |
          if [ -n "${{ github.run_started_at }}" ]; then
            start_epoch=$(date -d "${{ github.event.head_commit.timestamp }}" +%s 2>/dev/null || echo "$(date +%s)")
            end_time=$(date +%s)
            duration=$((end_time - start_epoch))
            echo "::notice title=Performance Tests Duration::Performance benchmarks completed in ${duration} seconds"
          fi
        shell: bash

  # Docker Security Scan (pinned versions, proper error handling)
  docker-security:
    name: Docker Build & Security Scan
    runs-on: ubuntu-latest
    needs: quality
    timeout-minutes: 15
    if: ${{ !contains(github.event.head_commit.message, '[skip docker]') && (github.event_name == 'schedule' || contains(github.event.head_commit.message, '[docker]') || github.event_name == 'push' && github.ref == 'refs/heads/master') }}
    permissions:
      contents: read
      security-events: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: false
          load: true
          tags: csfrace-scraper:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Run Trivy vulnerability scanner (fail on HIGH/CRITICAL)
        uses: aquasecurity/trivy-action@0.16.1  # Pinned version for security
        with:
          image-ref: 'csfrace-scraper:${{ github.sha }}'
          format: 'table'
          severity: 'HIGH,CRITICAL'
          exit-code: 1
          
      - name: Run Trivy vulnerability scanner (SARIF output)
        uses: aquasecurity/trivy-action@0.16.1
        if: always()
        with:
          image-ref: 'csfrace-scraper:${{ github.sha }}'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'HIGH,CRITICAL'
        continue-on-error: true

      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'
          category: trivy

      - name: Run Hadolint (Dockerfile linting)
        uses: hadolint/hadolint-action@v3.1.0
        with:
          dockerfile: Dockerfile
          format: sarif
          output-file: hadolint-results.sarif
        continue-on-error: true

      - name: Upload Hadolint results
        uses: github/codeql-action/upload-sarif@v3
        if: always() && hashFiles('hadolint-results.sarif') != ''
        with:
          sarif_file: hadolint-results.sarif
          category: hadolint

  # Dependency Review (PR only)
  dependency-review:
    name: Dependency Security Review
    runs-on: ubuntu-latest
    timeout-minutes: 5
    permissions:
      contents: read
      pull-requests: read
    
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          persist-credentials: false
          
      - name: Dependency Review
        uses: actions/dependency-review-action@v3
        with:
          fail-on-severity: moderate
          allow-licenses: Apache-2.0, BSD-2-Clause, BSD-3-Clause, ISC, MIT